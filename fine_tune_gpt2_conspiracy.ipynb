{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83d\ude80 Fine-Tuning GPT-2 to Generate Conspiracy Theories\n", "\n", "This notebook walks through how to fine-tune GPT-2 on a dataset of conspiracy-related news and Reddit posts using HuggingFace Transformers.\n", "\n", "\ud83d\udc49 **Requirements:** Free Google Colab GPU or local setup with GPU"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udce6 Install Dependencies\n", "!pip install transformers datasets accelerate --quiet"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udde0 Load Pretrained GPT-2\n", "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n", "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n", "model = GPT2LMHeadModel.from_pretrained('gpt2')"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcc1 Load and Prepare Dataset\n", "from datasets import load_dataset\n", "import json\n", "\n", "# Replace with your actual file or path to .json dataset\n", "with open('/content/conspiracy_data.json') as f:\n", "    data = json.load(f)\n", "\n", "texts = [entry['title'] + ' ' + entry['summary'] for entry in data]\n", "dataset = { 'text': texts }"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u270f\ufe0f Tokenize\n", "from datasets import Dataset\n", "\n", "ds = Dataset.from_dict(dataset)\n", "def tokenize(batch):\n", "    return tokenizer(batch['text'], truncation=True, padding='max_length', max_length=128)\n", "\n", "tokenized_ds = ds.map(tokenize, batched=True)\n", "tokenized_ds = tokenized_ds.train_test_split(test_size=0.1)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2699\ufe0f Training\n", "from transformers import Trainer, TrainingArguments\n", "\n", "training_args = TrainingArguments(\n", "    output_dir=\"gpt2_conspiracy_model\",\n", "    per_device_train_batch_size=2,\n", "    per_device_eval_batch_size=2,\n", "    num_train_epochs=3,\n", "    logging_steps=10,\n", "    save_steps=500,\n", "    evaluation_strategy=\"epoch\",\n", "    fp16=True\n", ")\n", "\n", "trainer = Trainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized_ds['train'],\n", "    eval_dataset=tokenized_ds['test']\n", ")\n", "\n", "trainer.train()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u2705 Once trained, you can generate new conspiracy theory texts using `model.generate()`!"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 4}