{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5769a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "scraper.py\n",
    "\n",
    "Web scraper for the Belief Spiral â€“ Conspiracy Recommender Engine project.\n",
    "Collects text snippets from:\n",
    "  - Reddit (r/conspiracy)\n",
    "  - Wikipedia (List of conspiracy theories)\n",
    "  - Gab (public posts)\n",
    "  - Satirical sites (e.g., The Onion)\n",
    "\n",
    "Requirements:\n",
    "  pip install praw requests beautifulsoup4 python-dotenv\n",
    "\n",
    "Usage:\n",
    "  1. Create a .env file with:\n",
    "       REDDIT_CLIENT_ID=your_client_id\n",
    "       REDDIT_CLIENT_SECRET=your_client_secret\n",
    "       REDDIT_USER_AGENT=BeliefSpiralScraper/0.1\n",
    "  2. Run: python scraper.py\n",
    "  3. Output: conspiracy_snippets.json\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "try:\n",
    "    import praw\n",
    "except ImportError:\n",
    "    raise ImportError(\"Please install praw: pip install praw\")\n",
    "\n",
    "# Load credentials from .env\n",
    "load_dotenv()\n",
    "REDDIT_CLIENT_ID = os.getenv(\"REDDIT_CLIENT_ID\")\n",
    "REDDIT_CLIENT_SECRET = os.getenv(\"REDDIT_CLIENT_SECRET\")\n",
    "REDDIT_USER_AGENT = os.getenv(\"REDDIT_USER_AGENT\", \"BeliefSpiralScraper/0.1\")\n",
    "\n",
    "# Initialize Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id=REDDIT_CLIENT_ID,\n",
    "    client_secret=REDDIT_CLIENT_SECRET,\n",
    "    user_agent=REDDIT_USER_AGENT\n",
    ")\n",
    "\n",
    "def scrape_reddit(subreddit_name: str, limit: int = 500):\n",
    "    \"\"\"Scrape titles, bodies, and comments from a subreddit.\"\"\"\n",
    "    snippets = []\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    for submission in subreddit.hot(limit=limit):\n",
    "        # Title & body\n",
    "        snippets.append(submission.title)\n",
    "        if submission.selftext:\n",
    "            snippets.append(submission.selftext)\n",
    "        # Comments\n",
    "        submission.comments.replace_more(limit=0)\n",
    "        for comment in submission.comments.list():\n",
    "            snippets.append(comment.body)\n",
    "        time.sleep(0.5)  # be kind to Reddit\n",
    "    return snippets\n",
    "\n",
    "def scrape_wikipedia(url: str = \"https://en.wikipedia.org/wiki/List_of_conspiracy_theories\"):\n",
    "    \"\"\"Scrape list items (with summaries) from a Wikipedia page.\"\"\"\n",
    "    snippets = []\n",
    "    resp = requests.get(url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    items = soup.select(\".mw-parser-output > ul > li\")\n",
    "    for li in items:\n",
    "        text = li.get_text(\" \", strip=True)\n",
    "        if len(text) > 60:\n",
    "            snippets.append(text)\n",
    "    return snippets\n",
    "\n",
    "def scrape_gab(topic_url: str = \"https://gab.com/explore\", limit: int = 300):\n",
    "    \"\"\"Scrape public posts from a Gab topic page.\"\"\"\n",
    "    snippets = []\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "    resp = requests.get(topic_url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    posts = soup.select(\"p.post__content\")\n",
    "    for p in posts[:limit]:\n",
    "        txt = p.get_text(\" \", strip=True)\n",
    "        if txt:\n",
    "            snippets.append(txt)\n",
    "    return snippets\n",
    "\n",
    "def scrape_satire(source_url: str = \"https://www.theonion.com/tag/conspiracy\", limit: int = 100):\n",
    "    \"\"\"Scrape satirical paragraphs to balance tone.\"\"\"\n",
    "    snippets = []\n",
    "    resp = requests.get(source_url)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "    paras = soup.select(\"div.js_post-content p\")\n",
    "    for p in paras[:limit]:\n",
    "        text = p.get_text(\" \", strip=True)\n",
    "        if text:\n",
    "            snippets.append(text)\n",
    "    return snippets\n",
    "\n",
    "def main():\n",
    "    print(\"Scraping Reddit...\")\n",
    "    reddit_snips = scrape_reddit(\"conspiracy\", limit=500)\n",
    "\n",
    "    print(\"Scraping Wikipedia...\")\n",
    "    wiki_snips = scrape_wikipedia()\n",
    "\n",
    "    print(\"Scraping Gab...\")\n",
    "    gab_snips = scrape_gab(limit=300)\n",
    "\n",
    "    print(\"Scraping satire...\")\n",
    "    sat_snips = scrape_satire(limit=200)\n",
    "\n",
    "    combined = set(reddit_snips + wiki_snips + gab_snips + sat_snips)\n",
    "    print(f\"Total unique snippets: {len(combined)}\")\n",
    "\n",
    "    with open(\"conspiracy_snippets.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(list(combined), f, ensure_ascii=False, indent=2)\n",
    "    print(\"Saved to conspiracy_snippets.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "# pip3 install praw requests beautifulsoup4 python-dotenv"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
